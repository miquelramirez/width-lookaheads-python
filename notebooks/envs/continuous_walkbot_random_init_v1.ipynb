{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import wizluk.envs\n",
    "from wizluk.policies import ContinuousZeroPolicy, ContinuousRandomPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = 100 # Horizon, 50 seconds of simulated time\n",
    "N = 10 # Number of Rollouts\n",
    "dt = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero baseline cost: -3929.835875230265\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('ContinuousWalkBot-RandomInit-v1')\n",
    "zero_baseline = np.zeros((N,1))\n",
    "\n",
    "env.seed(1337)\n",
    "zero_pi = ContinuousZeroPolicy()\n",
    "\n",
    "for k in range(N):\n",
    "    x = env.reset()\n",
    "    for s in range(H):\n",
    "        u = zero_pi.get_action(env, x)\n",
    "        x, reward, done, info = env.step(u)\n",
    "        zero_baseline[k] += reward\n",
    "        if done : break\n",
    "\n",
    "zero_baseline_cost = np.mean(zero_baseline)\n",
    "print(\"Zero baseline cost: {}\".format(zero_baseline_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('ContinuousWalkBot-RandomInit-v1')\n",
    "env.seed(1337)\n",
    "zero_pi = ContinuousZeroPolicy()\n",
    "\n",
    "x = env.reset()\n",
    "for s in range(H):\n",
    "    env.render()\n",
    "    time.sleep(dt)\n",
    "    u = zero_pi.get_action(env, x)\n",
    "    x, reward, done, info = env.step(u)\n",
    "    \n",
    "    if done : break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random baseline cost: -3433.5413504986127\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('ContinuousWalkBot-RandomInit-v1')\n",
    "np.random.seed(1337)\n",
    "env.seed(1337)\n",
    "\n",
    "random_baseline = np.zeros((N,1))\n",
    "random_pi = ContinuousRandomPolicy()\n",
    "\n",
    "for k in range(N):\n",
    "    x = env.reset()\n",
    "    for s in range(H):\n",
    "        u = random_pi.get_action(env, x)\n",
    "        x, reward, done, info = env.step(u)\n",
    "        random_baseline[k] += reward\n",
    "        if done : \n",
    "            break\n",
    "\n",
    "random_baseline_cost = np.mean(random_baseline)\n",
    "print(\"Random baseline cost: {}\".format(random_baseline_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('ContinuousWalkBot-RandomInit-v1')\n",
    "np.random.seed(1337)\n",
    "env.seed(1337)\n",
    "random_pi = ContinuousRandomPolicy()\n",
    "\n",
    "x = env.reset()\n",
    "for s in range(H):\n",
    "    env.render()\n",
    "    time.sleep(dt)\n",
    "    u = random_pi.get_action(env, x)\n",
    "    x, reward, done, info = env.step(u)\n",
    "    if done : break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained LQR Finite Horizon Policy (via MIQP-based MPC controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1337]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wizluk.envs.walker import WalkBot\n",
    "\n",
    "env = gym.make('ContinuousWalkBot-RandomInit-v1')\n",
    "np.random.seed(1337)\n",
    "env.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting the knowledge we need to provide in order to approach this problem with a MPC controller:\n",
    "\n",
    " - A precise definition of the target (goal) state $x_G$.\n",
    " - The constraints on \"inputs\" (control variables, action) and \"outputs\" (the actual state variables).\n",
    " - The $Q$ and $R$ matrices.\n",
    " - The horizon for the Receding Horizon Control problem ($10$).\n",
    " - And full access to the system dynamics (discretised state and input matrices, matching the time step of the simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.9       ]\n",
      " [1.08453996]]\n"
     ]
    }
   ],
   "source": [
    "x = env.reset()\n",
    "x_G = np.matrix( [ [8.0], [2.0], [0.0], [0.0]])\n",
    "model = WalkBot(1,'walker')\n",
    "\n",
    "model.add_bounds('x', 0.0,10.0)\n",
    "model.add_bounds('y', 0.0,10.0)\n",
    "model.add_bounds('vx', -10.0,10.0)\n",
    "model.add_bounds('vy', -10.0,10.0)\n",
    "model.add_bounds('ax', -2.0,2.0)\n",
    "model.add_bounds('ay', -2.0,2.0)\n",
    "\n",
    "u = model.mpc( x, x_G, 10, np.eye(4), np.eye(2), dt=dt)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Optimal\" cost: -493.84168627154423\n"
     ]
    }
   ],
   "source": [
    "from wizluk.envs.walker import WalkBot\n",
    "\n",
    "env = gym.make('ContinuousWalkBot-RandomInit-v1')\n",
    "np.random.seed(1337)\n",
    "env.seed(1337)\n",
    "\n",
    "mpc_cost = 0.0\n",
    "\n",
    "\n",
    "x = env.reset()\n",
    "for s in range(H):\n",
    "    env.render()\n",
    "    time.sleep(dt)\n",
    "    u = model.mpc( x, x_G, 10, np.eye(4), np.eye(2), dt=dt)\n",
    "    #print(u)\n",
    "    x, reward, done, info = env.step(u)\n",
    "    mpc_cost += reward\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "print('\"Optimal\" cost: {}'.format(mpc_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
